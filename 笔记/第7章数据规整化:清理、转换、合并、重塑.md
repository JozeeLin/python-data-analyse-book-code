# 数据规整化:清理、转换、合并、重塑

[TOC]

##合并数据集

>pandas对象中的数据合并方式:
>
>- pandas.merge可根据一个或多个键将不同DataFrame中的行连接起来。SQL或其他关系型数据库的用户对此应该会比较熟悉，因为它实现的就是数据库的连接操作
>- pandas.concat 可以沿着一条轴对多个对象堆叠到一起。
>- 实例方法combine_first可以将重复数据编接在一起，用一个对象中的值填充另一个对象中的缺失值。

#### 数据库风格的DataFrame合并

> 数据集的合并或连接运算是通过一个或多个键将行链接起来的。这些运算是关系型数据库的核心。

**merge函数的参数:**

- left——参与合并的左侧DataFrame
- right——参与合并的右侧DataFrame
- how——'inner','outer','left','right'其中一个。默认为'inner'
- on——用于连接的列名。必须存在于左右两个DataFrame对象中。如果未指定，且其他连接键也未指定，则以left和right列名的交集作为连接键
- left_on——左侧DataFrame中用作连接键的列
- right_on——右侧DataFrame中用作连接键
- left_index——将左侧的行索引用做其连接键
- right_index——类似于left_index
- sort——根据连接键对合并后的数据进行排序，默认为True。有时在处理大数据集时，禁用该选项可获得更好的性能
- suffixes——字符串值元组，用于追加到重叠列名的末尾，默认为('_ x', ‘_ y’)。例如，如果左右两个DataFrame对象都有‘data'，则结果中就会出现’data_x‘和'data_y'。
- copy——设置为False，可以在某些特殊情况下避免将数据复制到结果数据结构中。默认总是复制

```python
df1 = DataFrame({'key':['b','b','a','c','a','a','b'],
               'data1':range(7)})
df2 = DataFrame({'key':['a','b','d'],'data2':range(3)})
#默认是使用所有列名相同的列进行inner内部连接。没有相同的列名存在，报错(也就是连接列的交集)
pd.merge(df1,df2)
#最好指定一下使用哪一共同列来进行连接
pd.merge(df1, df2, on='key')
#两个数据集分别使用不同的列进行匹配连接
df3 = DataFrame({'1key':['b','b','a','c','a','a','b'],
                'data1':range(7)})
df4 = DataFrame({'rkey':['a','b','d'],
                'data2':range(3)})
pd.merge(df3, df4, left_on='1key', right_on='rkey')
#外部连接outer，连接列的并集
pd.merge(df1,df2, how='outer')
#以左边数据集的连接列为目标列来进行连接，#多对多连接产生的是行的笛卡尔积
df1 = DataFrame({'key':['b','b','a','c','a','b'],
                'data1':range(6)})
df2 = DataFrame({'key':['a','b','a','b','d'],
                'data2':range(5)})
pd.merge(df1,df2,on='key',how='left')
#以右边数据集的连接列为目标列来进行连接
pd.merge(df1,df2,on='key',how='right')
#指定多个共同列为连接列
left = DataFrame({'key1':['foo','foo','bar'],
                 'key2':['one','two','one'],
                 'lval':[1,2,3]})
right = DataFrame({'key1':['foo','foo','bar','bar'],
                  'key2':['one','one','one','two'],
                  'rval':[4,5,6,7]})
pd.merge(left, right, on=['key1','key2'], how='outer')
#自定义非连接列的公共列的后缀
pd.merge(left, right,on='key1', suffixes=('_left', '_right'))
```

#### 索引上的合并

```python
left1 = DataFrame({'key':['a','b','a','a','b','c'],
                  'value':range(6)})
right1 =DataFrame({'group_val':[2.5,7]}, index=['a','b'])
#左数据集的key列和右数据集的索引进行连接
pd.merge(left1, right1, left_on='key', right_index=True)

lefth = DataFrame({'key1':['Ohio', 'Ohio','Ohio', 'Nevada','Nevada'],
                  'key2':[2000,2001,2002,2001,2002],
                  'data':np.arange(5.)})
righth = DataFrame(np.arange(12).reshape((6,2)),
                  index=[['Nevada','Nevada','Ohio','Ohio','Ohio','Ohio'],
                        [2001,2000,2000,2000,2001,2002]],
                  columns = ['event1','event2'])
#层次化索引连接。左数据集的key1、key2列和右数据集的层级索引进行连接
pd.merge(lefth, righth, left_on=['key1','key2'], right_index=True)

left2 = DataFrame([[1,2],[3,4],[5,6]],index=['a','c','e'],columns=['Ohio','Nevada'])
right2 = DataFrame([[7,8],[9,10],[11,12],[13,14]],index=['b','c','d','e'],columns=['Missouri','Alabama'])
#行索引连接
pd.merge(left2, right2, how='outer', left_index=True, right_index=True)
#另外一种行索引连接，默认是左连接。可是设置how=right、inner、outer来改变连接方式
left2.join(right2, how='outer') #与上面的等效
#行索引和数据列进行连接，left1的行索引和right1的key列进行连接
left1.join(right1,on='key')
#多个数据集和调用者数据集进行连接
another = DataFrame([[7,8],[9,10],[11,12],[16,17]],index=['a','c','e','f'], columns=['New York', 'Oregon'])
left2.join([right2, another])
```

#### 轴向连接

**concat函数的参数:**

- objs——参与连接的pandas对象的列表或字典。唯一必需的参数
- axis——指明连接的轴向，默认为0
- join——'inner','outer'其中之一，默认为'outer'。指明其他轴向上的索引是按交集(inner)还是并集(outer)
- join_axes——指明用于其他n-1条轴的索引，不执行并集/交集运算
- keys——与连接对象有关的值，用于形成连接轴向上的层次化索引。可以是任意值的列表或数组、元组数组、数组列表(如果将levels设置成多级数组的话)
- levels——指定用作层次化索引各级别上的索引，如果设置了keys的话
- names——用于创建分层级别的名称，如果设置了keys和(或)levels的话
- verify_integrity——检查结果对象新轴上的重复情况，如果发现则引发异常。默认允许重复
- ignore_index——不保留连接轴上的索引，产生一组新索引range(total_length)

```python
arr = np.arange(12).reshape((3,4))
np.concatenate([arr,arr], axis=1)
s1 = Series([0,1], index=['a','b'])
s2 = Series([2,3,4], index=['c','d','e'])
s3 = Series([5,6], index=['f','g'])
#结果是Series数据，values为同一列的value，所有index连接起来不做任何的集合运算
pd.concat([s1,s2,s3])
#结果是DataFrame数据，3个参数的value对应3列，index做并集操作
pd.concat([s1,s2,s3], axis=1)
#DataFrame,index交集
pd.concat([s1,s4],axis=1,join='inner')
#DataFrame，指定index
pd.concat([s1,s4],axis=1,join_axes=[['a','c','b','e']])
#带有层次化索引的Series数据，第一级索引表示各个数据集的来源
result = pd.concat([s1,s1,s3], keys=['one','two','three'])
#DataFrame，各个数据集设置列名
pd.concat([s1,s2,s3],axis=1, keys=['one','two','three'])
#对DataFrame对象进行concat
df1 = DataFrame(np.arange(6).reshape(3,2), index=['a','b','c'], columns=['one','two'])
df2 = DataFrame(5+np.arange(4).reshape(2,2), index=['a','c'], columns=['three','four'])
#如果原来是有列名的，那么后来设置的列名升级为最高列名
pd.concat([df1,df2], axis=1, keys=['level1','level2'])
pd.concat({'level1':df1,'level2':df2}, axis=1) #效果同上
pd.concat([df1,df2], axis=1, keys=['level1','level2'], names=['upper','lower']) #设置列索引名称

#去掉没用的行索引
df1 = DataFrame(np.random.randn(3,4), columns=['a','b','c','d'])
df2 = DataFrame(np.random.randn(2,3), columns=['b','d','a'])
pd.concat([df1,df2], ignore_index=True)
```

#### 合并重叠数据

> 有两个数据集，索引全部或部分重叠，以其中一个数据集为主，用另外一个数据集中非NaN的重叠部分填充主要数据集缺失值。

```python
a = Series([np.nan,2.5,np.nan,3.5,4.5,np.nan], index=['f','e','d','c','b','a'])
b = Series(np.arange(len(a), dtype=np.float64), index=['f','e','d','c','b','a'])
b[-1] = np.nan
#使用numpy.where来实现
np.where(pd.isnull(a),b,a)
#也可以使用combine_first来实现
b[:-2].combine_first(a[2:])

df1 = DataFrame({'a':[1,np.nan,5,np.nan],
                'b':[np.nan,2,np.nan,6],
                'c':range(2,18,4)})
df2 = DataFrame({'a':[5,4,np.nan,3,7],
                'b':[np.nan, 3,4,6,8]})
df1.combine_first(df2)
```

## 重塑和轴向旋转

### 重塑层次化索引

- stack——将数据的列“旋转为行”
- unstack——将数据的行“旋转”为列

```python
data = DataFrame(np.arange(6).reshape((2,3)),
                index=pd.Index(['Ohio','Colorado'], name='state'),
                columns=pd.Index(['one','two','three'],name='number'))
#默认是把最底层的索引转成列索引
result = data.stack()
result.unstack()
#指定层级索引的编号或名称
result.unstack(0)
result.unstack('state')

s1 = Series([0,1,2,3], index=['a','b','c','d'])
s2 = Series([4,5,6], index=['c','d','e'])
data2 = pd.concat([s1,s2], keys=['one', 'two'])
data2.unstack()
#逆操作
data2.unstack().stack()
#默认会滤除缺失值，一来一去就把原始的缺失值去掉了，等效于执行dropna
data2.unstack().stack(dropna=False)

df = DataFrame({'left':result, 'right':result+5},
              columns=pd.Index(['left','right'], name='side'))
#作为旋转轴的级别将会成为结果中的最低级别
df.unstack('state')
df.unstack('state').stack('side')
```

### 将“长格式”旋转为“宽格式”

> 时间序列数据通常是以所谓的“长格式”或“堆叠格式”存储在数据库和CSV中

## 数据转换

### 移除重复数据

```python
data = DataFrame({'k1':['one']*3+['two']*4,
                 'k2':[1,1,2,3,3,4,4]})
#判断数据集中的是否存在重复数据，返回布尔数组
data.duplicated()
#删除掉重复的数据，默认只保留第一次出现的数据，后面再次出现完全一样的数据直接drop掉
data.drop_duplicates()

data['v1'] = range(7)
#只希望根据k1列过滤重复项
data.drop_duplicates(['k1'])
#保留重复项的最后一个数据，默认是保留第一数据
data.drop_duplicates(['k1','k2'], keep='last') 
```

### 利用函数或映射进行数据转换

```python
data = DataFrame({'food':['bacon','pulled pork','bacon','Pastrami',
                         'corned beef', 'Bacon','pastrami',
                          'honey ham','nova lox'],
                 'ounces':[4,3,12,6,7,8,3,5,6]})
meat_to_animal = {
    'bacon':'pig',
    'pulled pork':'pig',
    'pastrami':'cow',
    'corned beef':'cow',
    'honey ham':'pig',
    'nova lox':'salmon'
}
data['animal'] = data['food'].map(str.lower).map(meat_to_animal)
data['food'].map(lambda x: meat_to_animal[x.lower()])
```

### 值替换

```python
1、fillna填充缺失值，2、map修改对象中的数据子集，3、replace 替换对象中的数据子集
```

```python
data = Series([1,-999,2,-999,-1000,3])
#把值为-999的元素替换为np.nan
data.replace(-999,np.nan)
#把值为-999、-1000的元素都替换为np.nan
data.replace([-999,-1000],np.nan)
#把值为-999的替换为np.nan,-1000的替换为0
data.replace([-999,-1000],[np.nan,0])
data.replace({-999:np.nan,-1000:0}) #效果同上
```

### 重命名轴索引

```python
data = DataFrame(np.arange(12).reshape((3,4)),
                index=['Ohio','Colorado','New York'],
                columns=['one','two','three','four'])
#把行索引的所有的字符串变成大写字符串
data.index.map(str.upper)
#赋值的方式更改index
data.index=data.index.map(str.upper)
#使用内置函数改变行索引或列索引,
data.rename(index=str.title,columns=str.upper)
#使用字典数据来更改轴索引，默认返回新对象
data.rename(index={'OHIO':'INDIANA'},
           columns={'three':'peekaboo'})
#就地修改
data.rename(index={'OHIO':'INDIANA'},inplace=True)
```

### 离散化和面元划分

>为了便于分析，连续数据通常被离散化或拆分为“面元”(bin)

```python
ages =[20,22,25,27,21,23,37,32,61,45,41,32]
bins = [18,25,35,60,100]
#使用面元划分
cuts = pd.cut(ages, bins)
#为年龄数据进行标号的codes属性
cuts.codes
#表示不同分类名称的categories数组
cuts.categories
#对所有的区间进行统计
pd.value_counts(cats)
#指定闭区间在左边还是右边
pd.cut(ages, [18,26,36,61,100], right=False)

group_names = ['Youth', 'YoungAdult','MiddleAges','Senior']
#给每个分类(区间)设置一个别名
pd.cut(ages, bins, labels=group_names)

data = np.random.rand(20)
#指定面元数量，设置浮点数比较精度
pd.cut(data, 4, precision=2)
#根据样本分位数对数据进行面元划分
data = np.random.randn(1000)
cuts = pd.qcut(data, 4)
#自定义分位数
pd.qcut(data, [0,0.1,0.5,0.9,1])
```

### 检测和过滤异常值

```python
#含有正态分布的数据
np.random.seed(12345)
data = DataFrame(np.random.randn(1000,4))
data.describe()
#找出某列中绝对值大小超过3的值
col = data[3]
col[np.abs(col)>3]
#找出全部含有绝对值大小超过3的行
data[(np.abs(data)>3).any(1)]
```

### 排列和随机采样

> numpy.random.permutation函数可以轻松实现对Series或DataFrame的列的排列工作。

```python
df = DataFrame(np.arange(5*4).reshape(5,4))
#1-5个整数的全排列中的一种排列
sampler = np.random.permutation(5)
#对df数据按照sampler的顺序重新排列它的行数据
df.take(sampler)
#生成随机数组，可以重复的，可放回概率事件
bag = np.array([5,7,-1,6,4])
sampler = np.random.randint(0,len(bag), size=10)
draws = bag.take(sampler)
```

### 计算指标/哑变量

> 将"分类变量"转换为"哑变量矩阵"或"指标矩阵"。

```python
df = DataFrame({'key':['b','b','a','c','a','b'],
               'data1':range(6)})
#得到key列的哑变量矩阵
pd.get_dummies(df['key'])
#把生成的哑变量矩阵合并到旧的DF数据中，同时删掉原来的列
dummies = pd.get_dummies(df['key'], prefix='key') #指定哑变量矩阵列名前缀
df_with_dummy = df[['data1']].join(dummies)

#对电影分类列求哑变量矩阵
mnames = ['movie_id','title','genres']
movies = pd.read_table('movies.dat', sep='::',
                       header=None,names=mnames) #读入数据
genre_iter = (set(x.split('|')) for x in movies.genres) #把每一个电影的分类字符串切分
genres = sorted(set.union(*genre_iter)) #得到全部电影分类的集合
dummies = DataFrame(np.zeros((len(movies), len(genres))), 
                    columns=genres)#生成一个全0的哑变量矩阵
#根据数据集中的电影分类列对哑变量矩阵赋值
for i, gen in enumerate(movies.genres):
    dummies.loc[i, gen.split('|')] = 1
#把生成的哑变量矩阵连接到源数据中，并设置列名前缀
movies_windic = movies.join(dummies.add_prefix('Genre_'))
movies_windic.loc[0]

#求面元数据求哑变量矩阵
values = np.random.rand(10)
bins = [0,0.2,0.4,0.6,0.8,1]
pd.get_dummies(pd.cut(values, bins))
```

## 字符串操作

### 字符串对象方法

**python内置的字符串方法:**

- count——返回子串在字符串中的出现次数(非重叠)
- endswith、startswith——如果字符串以某个后缀结尾(以某个前缀开头)，则返回True
- join——将字符串用做连接其它字符串序列的分隔符
- index——如果在字符串中找到子串，则返回字串第一个字符所在的位置。如果没有找到，则引发ValueError
- find——如果在字符串中找到子串，则返回第一个发现的子串的地一个字符所在的位置。如果没有找到，则返回-1
- rfind——如果在字符串中找到子串，则返回最后一个发现的子串的第一个字符所在的位置。如果没有找到，则返回-1
- replace——用另一个字符串替换指定子串
- strip、rstrip、lstrip——去除空白符(包括换行符)。
- ljust、rjust——用空格或其他字符填充字符串的空白测以返回符合最低宽度的字符串

### pandas中矢量化的字符串函数

- cat——实现元素级的字符串连接操作，可指定分隔符
- contains——返回表示各字符串是否含有指定模式的布尔型数组
- count——模式的出现次数
- endswith、startswith
- pad——在字符串的左边、右边或左右两边添加空白字符
- center——相当于pad(side='both')
- repeat——重复值，s.str.repeat(3)
- slice——对Series中的各个字符串进行子串截取





