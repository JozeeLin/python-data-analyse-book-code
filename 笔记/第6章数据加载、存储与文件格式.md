# 数据加载、存储与文件格式

[TOC]

## 读写文本格式的数据

### 读写文件的函数

- read_csv 从文件、URL、文件型对象中加载带分隔符的数据。默认分隔符为逗号
- read_table 从文件、URL、文件型对象中加载带分隔符的数据。默认分隔符为制表符('\t')
- read_fwf 读取定宽列格式数据(也就是说，没有分隔符)
- read_clipboard 读取剪贴板中的数据，可以看作read_table的剪贴板版。在将网页转换为表格时很有用。

```python
#读写正常的csv文件
df = pd.read_csv('ch06/ex1.csv')
#使用read_table来读取csv文件，只要指定分隔符即可
pd.read_table('ch06/ex1.csv', sep=',')
#对于文件中没有列名信息的csv文件
pd.read_csv('ch06/ex2.csv',header=None)#可以通过设置header=None来设置默认的列名
pd.read_csv('ch06/ex2.csv',names=['a','b','c','d','message'])#也可以自定义一个列名数组
#设置指定的列作为行索引,可以设置多列
names = ['a','b','c','d','message']
pd.read_csv('ch06/ex2.csv',names=names, index_col=['message'])
#处理非常规分隔符文件。比如，不同长度的空格作为分隔符
result = pd.read_table('ch06/ex3.txt', sep='\s+')
#跳过指定的行，不对它们进行处理
pd.read_csv('ch06/ex4.csv', skiprows=[0,2,3])
#可以指定那些数值当作NaN值处理，可以根据不同的列来指定不同的数值(集)
result = pd.read_csv('ch06/ex5.csv', na_values=['NULL']) #表示所有的NULL值替换为NaN
sentinels = {'message':['foo','NA'], 'something':['two']}
pd.read_csv('ch06/ex5.csv', na_values=sentinels)#message列的foo、NA替换为NAN，something列的two值替换为NaN
```

### 函数选项

> - 索引：将一个或多个列当作返回的DataFrame处理，以及是否从文件、用户获取列名
> - 类型推断和数据转换：包括用户定义值的转换、缺失值标记列表等
> - 日期解析：包括组合功能，比如将分散在多个列中的日期时间信息组合成结果中的单个列
> - 迭代：支持对大文件进行逐块迭代
> - 不规整数据问题：跳过一些行、页脚、注释或其他一些不重要的东西(比如由成千上万个逗号隔开的数值数据)

**read_csv/read_table函数参数:**

- path ——表示文件系统的位置、URL、文件型对象的字符串
- sep/delimiter——用于对行中各字段进行拆分的字符序列或正则表达式
- header——用作列名的行号。默认为0(第一行)，如果没有header行就应该设置为None
- index_col——用作行索引的列编号或列名。可以是单个名称/数字或由多个名称/数字组成的列表(层次化索引)
- names——用于结果的列名列表，结合header=None
- skiprows——需要忽略的行数（从文件开始处算起），或需要跳过的行号列表（从0开始
- na_values——一组用于替换NA的值
- comment——用于将注释信息从行尾拆分出去的字符（一个或多个）
- parse_dates——尝试将数据解析为日期，默认为False。如果为True，则尝试解析所有列。此外，还可以指定需要解析的一组列号或列名。如果列表的元素为列表或元组，将会将多个列组合到一起再进行日期解析工作(例如，日期/时间分别位于两个列中)
- keep_date_col——如果连接多列解析日期，则保持参与连接的列。默认为False
- converters——由列号/列名跟函数之间的映射关系组成的字典。例如，{'foo':f}会对foo列的所有值应用函数f
- dayfirst——当解析有歧义的日期时，将其看作国际格式(例如,7/6/2012->June 7,2012)。默认为False
- date_parser——用于解析日期的函数
- nrows——需要读取的行数(从文件开始处算起)
- iterator——返回一个TextParser以便逐块读取文件
- chunksize——文件块的大小(用于迭代)
- skip_footer——需要忽略的行数(从文件末尾处算起)
- verbose——打印各种解析器输出信息，比如"非数值列中缺失值的数量"等
- encoding——用于unicode的文件编码格式。例如，'utf-8'表示用UTF-8编码的文本
- squeeze——如果数据经解析后仅含一列，则返回Series
- thousands——千分位分隔符，如','或'.'

### 逐块读取文本文件

```python
#只想读取前面几行，使用nrows进行指定
pd.read_csv('ch06/ex6.csv', nrows=5)
#可进行迭代式的读取数据，通过逐块读取的方式对key列进行sort_values操作
chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)
tot = pd.Series([])
for piece in chunker:
    tot = tot.add(piece['key'].value_counts(), fill_value=0)
tot = tot.sort_values(ascending=False)
```

### 将数据写到文本格式

```python
#正常的写入到一个文件中
data.to_csv('ch06/out.csv') #分隔符默认为逗号
#输出分隔符为'|'格式的数据
data.to_csv(sys.stdout, sep='|')
#NaN值替换为NULL
data.to_csv(sys.stdout, na_rep='NULL')
#输出数据中不显示行索引和列索引
data.to_csv(sys.stdout, index=False, header=False)
#重新赋值列索引/行索引
data.to_csv(sys.stdout, index=False, columns=['a','b','c'])
#将csv文件读取为Series
dates = pd.date_range('1/1/2000', periods=7)
ts = pd.Series(np.arange(7),index=dates)
ts.to_csv('ch06/tseries.csv')
pd.Series.from_csv('ch06/tseries.csv', parse_dates=True) #使用read_csv返回值为DataFrame
```

### 手工处理分隔符格式

```python
import csv
f = open('ch06/ex7.csv')
reader = csv.reader(f)
for line in reader:
    print line
lines = list(csv.reader(open('ch06/ex7.csv')))
header, values = lines[0], lines[1:]
data_dict = {h:v for h,v in zip(header, zip(*values))}
```

### JSON数据

```python
import json
obj = """{"name":"Wes",
        "places_lived":["United States","Spain", "Germany"],
        "pet":null,
        "siblings":[{"name":"Scott","age":25,"pet":"zuko"},
        {"name":"Katie","age":33,"pet":"Cisco"}]}"""
result = json.loads(obj)
asjson = json.dumps(result)
siblings = pd.DataFrame(result['siblings'], columns=['name','age'])
siblings
```

### XML和HTML：Web信息收集

```python
from lxml.html import parse
from urllib2 import urlopen
parsed = parse(urlopen('http://finance.yahoo.com/q/op?s=AAPL+Options'))
links = parsed.findall('.//a')
links[15:20]
lnk = links[28]
lnk.get('href')
lnk.text_content()
#获取文档中全部的URL
urls = [lnk.get('href') for lnk in parsed.findall('.//a')]
#获取文档中全部的表格列表
tables = parsed.findall('.//table')
calls = tables[0]
puts = tables[1]
#每个表格都有一个标题行，然后才是数据行
rows = calls.findall('.//tr')
#提取出标题行和数据行中单元格数据
def _unpack(row, kind='td'):
    elts = row.findall('.//%s' % kind)
    return [val.text_content() for val in elts]
#获取标题行中单元格数据列表
_unpack(rows[0], kind='th')
#获取数据行中单元格数据列表
_unpack(rows[1], kind='td')
#把获取到的表格数据转化成DataFrame格式
from pandas.io.parsers import TextParser
def parse_options_data(table):
    rows = table.findall('.//tr')
    header = _unpack(rows[0], kind='th')
    data = [_unpack(r) for r in rows[1:]]
    return TextParser(data, names=header).get_chunk()
#对call和put表格分别调用以上函数，把它们转化成DataFrame格式
call_data = parse_options_data(calls)
put_data = parse_options_data(puts)
call_data[:10]
```

####利用lxml.objectify解析XML

> XML是另一种常见的支持分层、嵌套数据以及元数据的结构化数据格式。

```python
from lxml import objectify
path = 'ch06/Performance_MNR.xml'
parsed = objectify.parse(open(path))
root = parsed.getroot()
data = []
skip_fields = ['PARENT_SEQ', 'INDICATOR_SEQ','DESIRED_CHANGE','DECIMAL_PLACES']
for elt in root.INDICATOR:
    el_data = {}
    for child in elt.getchildren():
        if child.tag in skip_fields:
            continue
        el_data[child.tag] = child.pyval
    data.append(el_data)
perf = pd.DataFrame(data)
```

##二进制数据格式

### 使用HDF5格式

> 高效读写磁盘上以二进制格式存储的科学数据。HDF5就是其中一个流行的工业级库。C库。HDF表示层级型数据格式。每个HDF5文件都含有一个文件系统式的节点结构，它使你能够存储多个数据集并支持元数据。支持多种压缩器的即时压缩，高效存储重复模式数据。对于非常大的无法直接读入内存的数据集，HDF5是不错的选择，可以高效地分块读写。
>
> python中有两个接口PyTables、h5py。h5py提供了一种直接而高级的HDF5 API访问接口，而PyTables则抽象了HDF5的许多细节以提供多种灵活的数据容器、表索引、查询功能以及对核外计算技术的某些支持。

### 读取Microsoft Excel文件

```python
xls_file = pd.ExcelFile('ch06/ex1.xlsx')
table = xls_file.parse('Sheet1')
```

## 使用HTML和Web API

> 许多网站都有一些通过JSON或其他格式提供数据的公共API。通过调用这些API来获取数据，并使用python来进行解析。

## 使用数据库

```python
import sqlite3
query = """CREATE TABLE test (a VARCHAR(20), b VARCHAR(20), c REAL, d INTEGER);"""
con = sqlite3.connect(':memory:')
con.execute(query)
con.commit()
data = [('Atlanta', 'Georgia', 1.25, 6),
       ('Tallahassee', 'Florida', 2.6, 3),
       ('Sacramento', 'California', 1.7, 5)]
stmt = 'INSERT INTO test VALUES(?,?,?,?)'

con.executemany(stmt, data)
con.commit()
#正常的数据库查询语句，返回一个列表
cursor = con.execute('select * from test')
rows = cursor.fetchall() #列数据
#还需要列名信息。它位于游标的description属性中
cursor.description
#把以上的到的两种数据组成一个完成的DataFrame数据
DataFrame(rows, columns=zip(*cursor.description)[0])
#直接生成DataFrame数据
import pandas.io.sql as sql
sql.read_sql('select * from test', con)
```

### 存取Mongodb中的数据

```python
import pymongo
con = pymongo.Connection('localhost', port=27017)
```

